{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet transformers\n!pip install --quiet sentencepiece\n!pip install --quiet datasets\n!pip install --quiet rouge_score","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:24:37.222962Z","iopub.execute_input":"2022-04-27T22:24:37.223268Z","iopub.status.idle":"2022-04-27T22:25:16.744108Z","shell.execute_reply.started":"2022-04-27T22:24:37.223235Z","shell.execute_reply":"2022-04-27T22:25:16.743300Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# importing libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd#toread csv file\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM#for model\nfrom transformers import AdamW, get_scheduler\nfrom datasets import load_metric\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.auto import tqdm\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nrcParams['figure.figsize'] = 16, 10\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:25:16.746274Z","iopub.execute_input":"2022-04-27T22:25:16.746751Z","iopub.status.idle":"2022-04-27T22:25:17.837281Z","shell.execute_reply.started":"2022-04-27T22:25:16.746709Z","shell.execute_reply":"2022-04-27T22:25:17.836354Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Making tockenizer and model","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:25:17.841912Z","iopub.execute_input":"2022-04-27T22:25:17.842167Z","iopub.status.idle":"2022-04-27T22:26:25.427712Z","shell.execute_reply.started":"2022-04-27T22:25:17.842134Z","shell.execute_reply":"2022-04-27T22:26:25.426932Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# importing dataset","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"../input/news-summary/news_summary.csv\",encoding = \"ISO-8859-1\")#encoding byte code\ndf.head()#seeing the first 5 rows","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:27:52.631485Z","iopub.execute_input":"2022-04-27T22:27:52.632286Z","iopub.status.idle":"2022-04-27T22:27:52.958419Z","shell.execute_reply.started":"2022-04-27T22:27:52.632242Z","shell.execute_reply":"2022-04-27T22:27:52.957660Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#defining the class summary \nclass summary_dataset:\n  #defining the constructor function with four default parameters\n  def __init__(self,\n               data=df,\n               tokenizer=tokenizer,\n               text_max_token_len=200,#means the text can have 200 max tokens\n               summary_max_token_len=12):#summary can have max 12 length\n               self.tokenizer = tokenizer#defining the parameter for tokenizing\n               self.data = data#taking the dataset \n               self.text_max_token_len = text_max_token_len\n               self.summary_max_token_len = summary_max_token_len\n  \n  #making function for length\n  def __len__(self):\n    return len(self.data)\n\n  #defining the get item for tokeninzing of text and then tokenizing of summary and return a dict as a result\n  def __getitem__(self,index:int): #specifying the index to get a specific row\n    #getting that specific row\n    data_row=self.data.iloc[index]\n    #taking text out of that row\n    text=data_row['text']\n\n    #we have now our text so we will tockenize it \n    text_encoding=tokenizer(\n        text,\n        max_length=self.text_max_token_len,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        add_special_tokens=True,\n        return_tensors='pt'\n\n    )\n    #Text encoding is done now so we will do summary encoding\n    summary_encoding=tokenizer(\n        text,\n        max_length=self.summary_max_token_len,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        add_special_tokens=True,\n        return_tensors='pt'\n\n    )\n    #taking out labels from the summary \n    labels = summary_encoding['input_ids']\n    labels[labels == tokenizer.pad_token_id] = -100\n    \n    return dict(\n        input_ids=text_encoding['input_ids'].flatten(),\n        attention_mask=text_encoding['attention_mask'].flatten(),\n        labels=labels.flatten(),\n        decoder_attention_mask=summary_encoding['attention_mask'].flatten())","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:29:30.280485Z","iopub.execute_input":"2022-04-27T22:29:30.280742Z","iopub.status.idle":"2022-04-27T22:29:30.291842Z","shell.execute_reply.started":"2022-04-27T22:29:30.280713Z","shell.execute_reply":"2022-04-27T22:29:30.290754Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n\n#splitting in the ratio of 80 and 20 \ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n#making the summary of the train and test\ntrain_dataset = summary_dataset(data=df_train)\ntest_dataset = summary_dataset(data=df_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:29:43.633718Z","iopub.execute_input":"2022-04-27T22:29:43.634438Z","iopub.status.idle":"2022-04-27T22:29:43.644230Z","shell.execute_reply.started":"2022-04-27T22:29:43.634401Z","shell.execute_reply":"2022-04-27T22:29:43.643461Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\n\ntrain_dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:29:53.803236Z","iopub.execute_input":"2022-04-27T22:29:53.804031Z","iopub.status.idle":"2022-04-27T22:29:53.809999Z","shell.execute_reply.started":"2022-04-27T22:29:53.803981Z","shell.execute_reply":"2022-04-27T22:29:53.809289Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#loading the data\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\neval_dataloader = DataLoader(test_dataset, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:30:30.796360Z","iopub.execute_input":"2022-04-27T22:30:30.796622Z","iopub.status.idle":"2022-04-27T22:30:30.801370Z","shell.execute_reply.started":"2022-04-27T22:30:30.796594Z","shell.execute_reply":"2022-04-27T22:30:30.800652Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#for less computation I will take epochs=3\n\nnum_epochs=3\n\n#calcualting training steps\nnum_training_steps=num_epochs*len(train_dataloader)\n\n#defining optimizer as ADAM\noptimizer=AdamW(model.parameters())\n\n#scheduling the learning rate using get_scheduler\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n#for seeing the progess bar \nprogress_bar = tqdm(range(num_training_steps))\n\n#fisrt checking for CUDA\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#for moving model to CUDA if available\nmodel = model.to(device)\n\n#now model is ready so doing the training part \nfor epoch in range(num_epochs):\n    #defining the batch in train data\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        #giving output\n        outputs = model(**batch)\n        #checking for loss\n        loss = outputs.loss\n        #backpropogating the loss\n        loss.backward()\n        #defining and scheduling the optimizer\n        optimizer.step()\n        lr_scheduler.step()\n        \n        #setting the gradient for all optimized tensor as 0\n        optimizer.zero_grad()\n        #seeing the progress bar\n        progress_bar.update()\n    #saving the trained model\n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            }, f'./t5-epoch-{epoch}.pth')\n    #printing the epochs\n    print(f'epoch: {epoch + 1} -- loss: {loss}')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T22:30:39.232963Z","iopub.execute_input":"2022-04-27T22:30:39.233483Z","iopub.status.idle":"2022-04-27T23:03:18.866692Z","shell.execute_reply.started":"2022-04-27T22:30:39.233449Z","shell.execute_reply":"2022-04-27T23:03:18.865635Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#loading the metic as rouge\nmetric= load_metric(\"rouge\")\n#strating evaluataion\nmodel.eval()\n#running according to batch \nfor batch in eval_dataloader:\n  #making the batch on test \n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n    #Taking the outputs\n    logits = outputs.logits\n    #making the predictions\n    predictions = torch.argmax(logits, dim=-1)\n    #adding to metrics \n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n#showing final metrics\nmetric.compute()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T23:03:30.513790Z","iopub.execute_input":"2022-04-27T23:03:30.514057Z","iopub.status.idle":"2022-04-27T23:03:59.026844Z","shell.execute_reply.started":"2022-04-27T23:03:30.514029Z","shell.execute_reply":"2022-04-27T23:03:59.026149Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Now we have our model so we can summarise our summary \ndef summarizeText(text, model=model):\n    text_encoding = tokenizer(\n        text,\n        max_length=1000,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        add_special_tokens=True,\n        return_tensors='pt'\n    )\n    #taking out generated ids according to model saved\n    generated_ids = model.generate(\n        input_ids=text_encoding['input_ids'].to(device),\n        attention_mask=text_encoding['attention_mask'].to(device),\n        max_length=150,\n        num_beams=4,\n        repetition_penalty=2.5,\n        length_penalty=1.0,\n        early_stopping=True\n    )    \n    #taking out the predictions\n    preds = [\n            tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n            for gen_id in generated_ids\n    ]\n    #returning the predictions\n    return \"\".join(preds)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T23:04:17.559919Z","iopub.execute_input":"2022-04-27T23:04:17.560191Z","iopub.status.idle":"2022-04-27T23:04:17.567466Z","shell.execute_reply.started":"2022-04-27T23:04:17.560161Z","shell.execute_reply":"2022-04-27T23:04:17.566446Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\n\n#we have now our text in summary \ntext = \"\"\"Russia has cut off natural gas supplies to Poland and Bulgaria, dramatically escalating its response to Western sanctions imposed on Moscow over the war in Ukraine. \"\"\"\n#summariesing the text in English\nsummary = summarizeText(text, model)\n\nprint(summary)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T23:04:24.582724Z","iopub.execute_input":"2022-04-27T23:04:24.583135Z","iopub.status.idle":"2022-04-27T23:04:25.030525Z","shell.execute_reply.started":"2022-04-27T23:04:24.583102Z","shell.execute_reply":"2022-04-27T23:04:25.029736Z"},"trusted":true},"execution_count":21,"outputs":[]}]}